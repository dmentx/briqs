#!/usr/bin/env python3
"""
Test script for Ollama integration with Multi-Agent Negotiation System
Validates local Ollama connectivity and runs a basic negotiation scenario
"""

import sys
import os
import requests
from pathlib import Path

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_ollama_connectivity():
    """Test if Ollama is running and accessible."""
    print("üîç Testing Ollama connectivity...")
    
    try:
        # Test basic Ollama API connectivity
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json()
            print("‚úÖ Ollama is running and accessible")
            
            # Check available models
            model_names = [model["name"] for model in models.get("models", [])]
            print(f"üìã Available models: {model_names}")
            
            # Check for our target models
            if "llama4:16x17b" in model_names:
                print("‚úÖ llama4:16x17b is available")
                return "llama4:16x17b"
            elif any("llama3.1" in name and "70b" in name for name in model_names):
                llama31_model = next(name for name in model_names if "llama3.1" in name and "70b" in name)
                print(f"‚úÖ Found Llama 3.1 70B model: {llama31_model}")
                return llama31_model
            elif any("llama" in name for name in model_names):
                llama_model = next(name for name in model_names if "llama" in name)
                print(f"‚ö†Ô∏è  Using fallback Llama model: {llama_model}")
                return llama_model
            else:
                print("‚ùå No suitable Llama models found")
                print("   Run: ollama pull llama3.1:70b")
                print("   Or:  ollama pull llama4:16x17b")
                return None
                
        else:
            print(f"‚ùå Ollama API responded with status {response.status_code}")
            return None
            
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Ollama. Is it running?")
        print("   Start Ollama with: ollama serve")
        return None
    except Exception as e:
        print(f"‚ùå Error testing Ollama connectivity: {e}")
        return None

def test_embedding_model():
    """Test embedding model availability (skipped - not needed)."""
    print("\nüîç Testing embedding model availability...")
    print("‚úÖ Embedding model not required - skipping")
    return True

def test_simple_llm_call():
    """Test a simple LLM call to verify Ollama integration."""
    print("\nüîç Testing simple LLM call...")
    
    try:
        from crewai import LLM
        
        # Try the specific model first
        try:
            llm = LLM(
                model="ollama/llama4:16x17b",
                base_url="http://localhost:11434"
            )
            response = llm.call("Hello! Please respond with 'Ollama integration working' if you can understand this.")
            print("‚úÖ llama4:16x17b LLM call successful")
            print(f"   Response: {response[:100]}...")
            return True
        except Exception:
            # Fallback to llama3.1
            try:
                llm = LLM(
                    model="ollama/llama3.1:70b",
                    base_url="http://localhost:11434"
                )
                response = llm.call("Hello! Please respond with 'Ollama integration working' if you can understand this.")
                print("‚úÖ llama3.1:70b LLM call successful")
                print(f"   Response: {response[:100]}...")
                return True
            except Exception as e:
                print(f"‚ùå LLM call failed: {e}")
                return False
                
    except ImportError as e:
        print(f"‚ùå Failed to import CrewAI LLM: {e}")
        return False

def test_crew_instantiation():
    """Test that the negotiation crew can be created with Ollama."""
    print("\nüîç Testing crew instantiation with Ollama...")
    
    try:
        from crew_ai.crew import create_negotiation_crew
        
        crew = create_negotiation_crew()
        print("‚úÖ Negotiation crew instantiated successfully with Ollama")
        
        # Test context setting
        test_context = {
            "contract_type": "software_license",
            "buyer_budget": 10000.0,
            "seller_price": 12000.0,
            "requirements": {
                "license_duration": "1 year",
                "support_level": "basic"
            }
        }
        
        crew.set_negotiation_context(test_context)
        print("‚úÖ Negotiation context set successfully")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to instantiate crew with Ollama: {e}")
        return False

def test_simple_negotiation():
    """Run a minimal negotiation scenario to test the full pipeline."""
    print("\nüîç Testing simple negotiation scenario...")
    
    try:
        from crew_ai.crew import create_negotiation_crew
        
        # Create crew
        crew = create_negotiation_crew()
        
        # Run a simple negotiation
        print("üöÄ Starting negotiation test...")
        result = crew.run_negotiation(
            contract_type="test_contract",
            buyer_budget=5000.0,
            seller_price=6000.0,
            requirements={
                "duration": "6 months",
                "support": "basic"
            }
        )
        
        if result.get("success"):
            print("‚úÖ Simple negotiation completed successfully!")
            print(f"   Negotiation ID: {result['context']['negotiation_id']}")
            return True
        else:
            print(f"‚ùå Negotiation failed: {result.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error running simple negotiation: {e}")
        return False

def main():
    """Run all Ollama integration tests."""
    print("üöÄ Multi-Agent Negotiation System - Ollama Integration Test")
    print("="*70)
    
    tests = [
        ("Ollama Connectivity", test_ollama_connectivity),
        ("Embedding Model (Skipped)", test_embedding_model),
        ("Simple LLM Call", test_simple_llm_call),
        ("Crew Instantiation", test_crew_instantiation),
        ("Simple Negotiation", test_simple_negotiation)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                failed += 1
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            failed += 1
            print(f"‚ùå {test_name} FAILED with exception: {e}")
    
    print("\n" + "="*70)
    print(f"üìä Test Results: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("üéâ All tests passed! Ollama integration is working correctly.")
        print("\nüöÄ Ready to run full negotiation scenarios!")
        print("   Try: uv run python -c \"from src.crew_ai.crew import run_sample_negotiation; print(run_sample_negotiation())\"")
    else:
        print("‚ö†Ô∏è  Some tests failed. Please review the output above.")
        print("\nüîß Setup Instructions:")
        print("   1. Start Ollama: ollama serve")
        print("   2. Pull models: ollama pull llama4:16x17b (or llama3.1:70b)")
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main()) 